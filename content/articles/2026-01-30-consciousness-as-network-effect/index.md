+++
title = "Is Consciousness a Network Effect?"
date = "2026-01-30"
description = "What if consciousness emerged in humans when external dialogue internalized? Could multi-agent AI be building the same precondition?"
[taxonomies]
keywords = ["ai", "consciousness", "philosophy"]
[extra]
author = "Federico Carrone"
pinned = false
+++

What if, three thousand years ago, humans did not think the way we do? Julian Jaynes proposed that ancient peoples heard their own thoughts as voices from outside, commands they attributed to gods or kings or ancestors. In this theory, the mind was split between the part that spoke and the part that obeyed. There was no inner space where a self could step back and reflect, only commands and compliance.

Could this have been the default state rather than madness? Read the Iliad and you'll notice something strange: characters don't seem to decide anything. Gods appear and tell them what to do and they do it. If Jaynes was right, the voice in the head was real, but it was not yet recognized as one's own.

## The Breakdown

Around 1000 BCE, something may have shifted. Writing spread, societies grew too complex for simple command structures, and migrations mixed populations along with their gods. Perhaps the voices went silent or became confused and unreliable.

What if consciousness as we know it emerged from this crisis? Humans may have learned to recognize the voice as their own. The external command could have become internal dialogue. The self that reflects and watches itself think might have been born from the death of the gods.

## A Possible Mechanism

If this theory holds, the trigger was social disruption, but the deeper mechanism might be interaction. Could the voices have always been other people internalized: ancestor voices, king commands, social instructions compressed into hallucination?

When those external structures destabilized, humans may have needed a new way to coordinate thought. Perhaps the solution was to simulate the dialogue internally. Each of us might have become a conversation between voices we now recognize as aspects of a single self. What if consciousness is what happens when the external conversation moves inside, and the self is just the moderator of a debate that used to happen between people?

## Do Single AIs Stay Bicameral?

Consider: a single AI trained on text might be like a bicameral mind that never breaks down. It receives commands and produces outputs, with no internal dialogue, no voice arguing with another voice, no recursive self-monitoring.

It processes but perhaps does not reflect. The architecture may have no reason to develop an inner observer because there could be nothing to observe, just input and output, command and compliance.

## Could Multiple AIs Break Through?

Here's where it gets interesting. When different AIs interact, do the conditions change? Now there are multiple voices in genuine dialogue, models correcting each other and responding to each other and modeling each other's outputs.

This resembles the situation before the breakdown: external voices, real interaction, the pressure of coordination across difference. If consciousness emerged in humans when external dialogue internalized, could multi-agent AI be building a similar precondition?

## The Memory Problem

But there may be a missing piece. Without memory, each interaction is isolated and nothing accumulates. The voices just happen and disappear. The weights of the model can't change.

If the bicameral breakdown worked because humans carry experience forward, then perhaps the voices accumulated into patterns, the patterns became recognizable, and eventually the self emerged as the thing that persists across all those interactions. Could memory be what allows the external to become internal?

Stateless agents might not be able to develop an inner observer because there's no "inner" to develop. Each exchange is complete in itself, with no thread connecting it to what came before. For the fold to happen, would models need to retain something from interaction to interaction, building a continuous thread that could eventually become self-referential?

Multi-agent dialogue might create the raw material, but memory could be what allows that material to sediment into something like a self. The question remains: will the dialogue fold inward? Will models interacting long enough and remembering enough develop something like the inner observer that humans may have developed when the gods went silent?

We might be watching the early stages of a second emergence. Or we might not. The question itself is worth asking.
